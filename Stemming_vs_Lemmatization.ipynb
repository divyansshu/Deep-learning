{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNlxMc1F34wjTsS+zxLTpmW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/divyansshu/Deep-learning/blob/main/Stemming_vs_Lemmatization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6K9jZFDrmRYe",
        "outputId": "1ab19d62-c34b-4448-92f8-fffd426bedb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemming = PorterStemmer()"
      ],
      "metadata": {
        "id": "1OS50AxG_Mp4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['caresses', 'flies', 'dies', 'mules', 'denied',\n",
        "            'died', 'agreed', 'owned', 'humbled', 'sized',\n",
        "            'meeting', 'stating', 'siezing', 'itemization',\n",
        "            'sensational', 'traditional', 'reference', 'colonizer',\n",
        "            'plotted']\n",
        "for word in words:\n",
        "  print(word+' ---->' +stemming.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCp_vgGN_PJr",
        "outputId": "1f0ec7df-e9f6-4d2a-f5db-e626c0866cd1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "caresses ---->caress\n",
            "flies ---->fli\n",
            "dies ---->die\n",
            "mules ---->mule\n",
            "denied ---->deni\n",
            "died ---->die\n",
            "agreed ---->agre\n",
            "owned ---->own\n",
            "humbled ---->humbl\n",
            "sized ---->size\n",
            "meeting ---->meet\n",
            "stating ---->state\n",
            "siezing ---->siez\n",
            "itemization ---->item\n",
            "sensational ---->sensat\n",
            "traditional ---->tradit\n",
            "reference ---->refer\n",
            "colonizer ---->colon\n",
            "plotted ---->plot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# stemming words from sentences"
      ],
      "metadata": {
        "id": "PS7KXP3TN0DM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "Ncq-gPHhAlSo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'Programmers program with programming languages'\n",
        "words = word_tokenize(sentence)\n",
        "for w in words:\n",
        "  print(w+'------>'+stemming.stem(w))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szmIvuUeOMi6",
        "outputId": "fab7546a-fff1-41e3-a4ff-7e916bcecb3b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Programmers------>programm\n",
            "program------>program\n",
            "with------>with\n",
            "programming------>program\n",
            "languages------>languag\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# snowboll stemmer"
      ],
      "metadata": {
        "id": "xeSOMiLePHkY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import SnowballStemmer"
      ],
      "metadata": {
        "id": "kKqDRL3rOf49"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# languages supported\n",
        "print(', '.join(SnowballStemmer.languages))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTh0LjOoPMvu",
        "outputId": "cc880ffc-dde8-4717-dd68-f0452bf43f1c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "arabic, danish, dutch, english, finnish, french, german, hungarian, italian, norwegian, porter, portuguese, romanian, russian, spanish, swedish\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJ87tPISQCgG",
        "outputId": "10fc08df-d06f-47c5-e24f-c871aa83034a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['caresses', 'flies', 'dies', 'mules', 'denied',\n",
        "            'died', 'agreed', 'owned', 'humbled', 'sized',\n",
        "            'meeting', 'stating', 'siezing', 'itemization',\n",
        "            'sensational', 'traditional', 'reference', 'colonizer',\n",
        "            'plotted']\n",
        "stemmer = SnowballStemmer('english', ignore_stopwords=True)\n",
        "for w in words:\n",
        "  print(w+'----->'+stemmer.stem(w))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Av5QURDtPRGc",
        "outputId": "ec851d91-d51f-46de-fd13-3fb3e32b6f09"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "caresses----->caress\n",
            "flies----->fli\n",
            "dies----->die\n",
            "mules----->mule\n",
            "denied----->deni\n",
            "died----->die\n",
            "agreed----->agre\n",
            "owned----->own\n",
            "humbled----->humbl\n",
            "sized----->size\n",
            "meeting----->meet\n",
            "stating----->state\n",
            "siezing----->siez\n",
            "itemization----->item\n",
            "sensational----->sensat\n",
            "traditional----->tradit\n",
            "reference----->refer\n",
            "colonizer----->colon\n",
            "plotted----->plot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## the english stemmer is better than the original 'porter' stemmer"
      ],
      "metadata": {
        "id": "n_-A7QwjQMtx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmatization"
      ],
      "metadata": {
        "id": "RC77HBmiQfAH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# parameters\n",
        "* word (str) – The input word to lemmatize.\n",
        "\n",
        "* pos (str) – The Part Of Speech tag. Valid options are “n” for nouns, “v” for verbs, “a” for adjectives, “r” for adverbs and “s” for satellite adjectives.\n",
        "\n"
      ],
      "metadata": {
        "id": "Ekl8feniaxMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print('rocks: ', lemmatizer.lemmatize('rocks'))\n",
        "print('copora: ', lemmatizer.lemmatize('corpora'))\n",
        "\n",
        "# 'a' denotes adjective in 'pos' (part of speech)\n",
        "print('better: ', lemmatizer.lemmatize('better', pos='a'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqSkNKqlPzwj",
        "outputId": "e17da85b-c676-42c5-b59c-18d2bab20362"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rocks:  rock\n",
            "copora:  corpus\n",
            "better:  good\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "# load the spaCy english model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "text = \"The quick brown foxes are jumping over the lazy dogs.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "lemmatized_tokens = [token.lemma_ for token in doc]\n",
        "lemmatized_text = ' '.join(lemmatized_tokens)\n",
        "\n",
        "print(\"Original Text:\", text)\n",
        "print(\"Lemmatized Text:\", lemmatized_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hb6gIGKBYqbj",
        "outputId": "c10cd78d-8752-40ac-b1db-55edcff456a3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: The quick brown foxes are jumping over the lazy dogs.\n",
            "Lemmatized Text: the quick brown fox be jump over the lazy dog .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_sentence = \"Python programmers often tend like programming in python because it's like english. We call people who program in python pythonistas.\"\n",
        "\n",
        "word_tokens = word_tokenize(example_sentence)\n",
        "\n",
        "# Perform lemmatization\n",
        "for word in word_tokens:\n",
        "   print (word+\"--->\"+lemmatizer.lemmatize(word, pos=\"v\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_gEJgKiaYuU",
        "outputId": "b8812b45-840a-4e42-d43f-43d1cd14117d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python--->Python\n",
            "programmers--->programmers\n",
            "often--->often\n",
            "tend--->tend\n",
            "like--->like\n",
            "programming--->program\n",
            "in--->in\n",
            "python--->python\n",
            "because--->because\n",
            "it--->it\n",
            "'s--->'s\n",
            "like--->like\n",
            "english--->english\n",
            ".--->.\n",
            "We--->We\n",
            "call--->call\n",
            "people--->people\n",
            "who--->who\n",
            "program--->program\n",
            "in--->in\n",
            "python--->python\n",
            "pythonistas--->pythonistas\n",
            ".--->.\n"
          ]
        }
      ]
    }
  ]
}